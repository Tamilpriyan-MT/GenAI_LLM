{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904ab90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f26e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Everyone.',\n",
       " 'Welcome to GeeksforGeeks.',\n",
       " 'You are studying an NLP article.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "text = \"Hello Everyone. Welcome to GeeksforGeeks. You are studying an NLP article.\"\n",
    "sent_tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9eecf6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences:\n",
      "1. Hi This is MT.\n",
      "2. I am an Artist.\n",
      "3. I have composed many songs.\n",
      "4. I love music.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hi This is MT. I am an Artist. I have composed many songs. I love music.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c987d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. \n",
      "Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and humans through language.\n",
      "2. The ultimate goal is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
      "3. NLP is used in applications like chatbots, translation, sentiment analysis, and more.\n",
      "4. In recent years, NLP technologies have become more advanced.\n",
      "5. Models like GPT and BERT are capable of understanding complex contexts and generating human-like responses.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and humans through language. \n",
    "The ultimate goal is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
    "NLP is used in applications like chatbots, translation, sentiment analysis, and more.\n",
    "In recent years, NLP technologies have become more advanced. Models like GPT and BERT are capable of understanding complex contexts and generating human-like responses.\n",
    "\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f2392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. Hi this is MT.\n",
      "2. I love Music.\n",
      "3. I am Music Director\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "with open(\"sample.txt\",\"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d569ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in English:\n",
      "1. \n",
      "Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and humans through language.\n",
      "2. The ultimate goal is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
      "3. NLP is used in applications like chatbots, translation, sentiment analysis, and more.\n",
      "4. In recent years, NLP technologies have become more advanced.\n",
      "5. Models like GPT and BERT are capable of understanding complex contexts and generating human-like responses.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and humans through language. \n",
    "The ultimate goal is to enable machines to understand, interpret, and generate human language in a way that is valuable.\n",
    "NLP is used in applications like chatbots, translation, sentiment analysis, and more.\n",
    "In recent years, NLP technologies have become more advanced. Models like GPT and BERT are capable of understanding complex contexts and generating human-like responses.\n",
    "\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text,language ='english')\n",
    "\n",
    "print(\"Sentences in English:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9220cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK sent_Tokenize:\n",
      "1. Hi This is MT.\n",
      "2. I am an Artist.\n",
      "3. I have composed many songs?\n",
      "4. love music.\n",
      "\n",
      "Native split by period:\n",
      "1.Hi This is MT. I am an Artist. I have composed many songs\n",
      "2.love music.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hi This is MT. I am an Artist. I have composed many songs?  love music.\"\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "native_seqences = text.split('?')\n",
    "print(\"NLTK sent_Tokenize:\")\n",
    "for i, sentence in enumerate(nltk_sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "print(\"\\nNative split by period:\")\n",
    "for i, sentence in enumerate(native_seqences ,1):\n",
    "    print(f\"{i}.{sentence.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092f53ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #for tokenization\n",
    "nltk.download('wordnet') #for lemmatization\n",
    "nltk.download('averaged_perceptron_tagger') # for POS taging(optional,but improve accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27dfb443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['running', 'flies', 'cats', 'better', 'geese', 'peoples', 'wives', 'lives']\n",
      "Lemmatized words:  ['running', 'fly', 'cat', 'better', 'goose', 'people', 'wife', 'life']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['running','flies','cats','better','geese','peoples','wives','lives']\n",
    "lemmatized_words=[lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Original words: \", words)\n",
    "print(\"Lemmatized words: \",lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e856c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"V\"):\n",
    "        return \"v\" \n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return \"n\" \n",
    "    elif treebank_tag.startswith(\"J\"):\n",
    "        return \"a\"  \n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return \"r\"  \n",
    "    else:\n",
    "        return 'n'  \n",
    "text = \"The cats are running faster and jumping higher.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "lemmatized_words = []\n",
    "for word, pos in pos_tags:\n",
    "    wordnet_pos = get_wordnet_pos(pos)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    lemmatized_words.append(lemmatized)\n",
    "\n",
    "print(\"Original words:\", tokens)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef343fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['Hi', 'this', 'is', 'MT', '.', 'I', 'love', 'Music', '.', 'I', 'am', 'Music', 'Director']\n",
      "Lemmatized words:  ['Hi', 'this', 'is', 'MT', '.', 'I', 'love', 'Music', '.', 'I', 'am', 'Music', 'Director']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "with open(\"sample.txt\",'r')as file:\n",
    "    text=file.read()\n",
    "words= word_tokenize(text)\n",
    "lemmatized_words=[lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Original words: \", words)\n",
    "print(\"Lemmatized words: \",lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89182f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed word: run\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "word=\"running\"\n",
    "stem=stemmer.stem(word)\n",
    "print(f\"Stemmed word: {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cd995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\tamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  The cats are playing with the toys. They were running quickly in the park.\n",
      "Tokenized words:  ['The', 'cats', 'are', 'playing', 'with', 'the', 'toys', '.', 'They', 'were', 'running', 'quickly', 'in', 'the', 'park', '.']\n",
      "Stemmed words:  ['the', 'cat', 'are', 'play', 'with', 'the', 'toy', '.', 'they', 'were', 'run', 'quickli', 'in', 'the', 'park', '.']\n",
      "Lemmatized words:  ['The', 'cat', 'be', 'play', 'with', 'the', 'toy', '.', 'They', 'be', 'run', 'quickly', 'in', 'the', 'park', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Initialize the lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define some example sentences (instead of reading from a file)\n",
    "text = \"The cats are playing with the toys. They were running quickly in the park.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Function to map POS tags to WordNet's format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# POS tagging to get the word types\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "# Lemmatize words using POS tags\n",
    "lemmatized_words = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words\n",
    "]\n",
    "\n",
    "# Stem words using Porter Stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print out the results\n",
    "print(\"Original text: \", text)\n",
    "print(\"Tokenized words: \", words)\n",
    "print(\"Stemmed words: \", stemmed_words)\n",
    "print(\"Lemmatized words: \", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f6e98f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The children are playing fairly and studying.\n",
      "Porter Stemmed: ['the', 'children', 'are', 'play', 'fairli', 'and', 'studi', '.']\n",
      "Snowball Stemmed: ['the', 'children', 'are', 'play', 'fair', 'and', 'studi', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\") #Specify Language\n",
    "text = \"The children are playing fairly and studying.\"\n",
    "words = word_tokenize(text)\n",
    "porter_stemmed = [porter.stem(word) for word in words]\n",
    "snowball_stemmed = [snowball.stem(word) for word in words]\n",
    "print(\"Original text:\", text)\n",
    "print(\"Porter Stemmed:\", porter_stemmed)\n",
    "print(\"Snowball Stemmed:\", snowball_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eee677b-0d22-40c4-ae55-c27300b18c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m LancasterStemmer()\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dogs were barking loudly. She enjoys running daily\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m(text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences,\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      8\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Initialize the Lancaster stemmers\n",
    "stemmer = LancasterStemmer()\n",
    "text = \"The dogs were barking loudly. She enjoys running daily\"\n",
    "sentences = sent_tokenize(text)\n",
    "for i, sentence in enumerate(sentences,1):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    print(f\"Sentence{i}:\")\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Stemmed :\",\"\".join(stemmed_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b51a5-11fd-4ac9-94eb-5c79273dcb4c",
   "metadata": {},
   "source": [
    "<!-- NLP techniques\n",
    "\n",
    "Text preprocessing in NLP\n",
    "Syntax and Parsing\n",
    "Symantics Analytics \n",
    "Information Extraction \n",
    "Text classification\n",
    "Language generation\n",
    "Speech Processing\n",
    "Q/A\n",
    "Dialogue System\n",
    "Sentiment Analysis -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18da1057-46e2-4365-ad9b-40d8bdacfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Techniques\n",
    "\n",
    "# Text Preprocessing\n",
    "# syntax and parsing\n",
    "# symatincs analysis\n",
    "# text classification\n",
    "# language generation\n",
    "# info extraction\n",
    "# speech processing\n",
    "# Q/A\n",
    "# Dialogue System\n",
    "# Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adb9e2-972c-4713-8791-7cc32b181d94",
   "metadata": {},
   "source": [
    "# OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd479760-5fff-4687-aef8-1961cc2268db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an Image\n",
    "image = cv2.imread('MT.png')\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Could not load image.\")\n",
    "else:\n",
    "    # Apply Gaussian Blur\n",
    "    blur = cv2.GaussianBlur(image, (15,15), 0)  # (15,15) is the kernel size\n",
    "\n",
    "    # Show Original and Blurred Images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"Grey Image:\", gray)\n",
    "    edge = cv2.Canny(image,100,200)\n",
    "    cv2.imshow(\"Edge Image\", edge)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Blurred Image\", blur)\n",
    "\n",
    "    # Wait for a key press and close the window\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a0a727-5abc-4957-a2a6-c778e958c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tamil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.0 MB 243.1 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 0.5/10.0 MB 243.1 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 0.5/10.0 MB 243.1 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 0.5/10.0 MB 243.1 kB/s eta 0:00:39\n",
      "   --- ------------------------------------ 0.8/10.0 MB 264.6 kB/s eta 0:00:35\n",
      "   --- ------------------------------------ 0.8/10.0 MB 264.6 kB/s eta 0:00:35\n",
      "   --- ------------------------------------ 0.8/10.0 MB 264.6 kB/s eta 0:00:35\n",
      "   --- ------------------------------------ 0.8/10.0 MB 264.6 kB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 293.6 kB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 293.6 kB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 322.3 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 322.3 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 322.3 kB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 322.3 kB/s eta 0:00:27\n",
      "   ------ --------------------------------- 1.6/10.0 MB 328.1 kB/s eta 0:00:26\n",
      "   ------ --------------------------------- 1.6/10.0 MB 328.1 kB/s eta 0:00:26\n",
      "   ------- -------------------------------- 1.8/10.0 MB 353.8 kB/s eta 0:00:23\n",
      "   ------- -------------------------------- 1.8/10.0 MB 353.8 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 2.1/10.0 MB 387.9 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.4/10.0 MB 411.0 kB/s eta 0:00:19\n",
      "   --------- ------------------------------ 2.4/10.0 MB 411.0 kB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 426.0 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 426.0 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 441.7 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 441.7 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.1/10.0 MB 453.5 kB/s eta 0:00:16\n",
      "   ------------ --------------------------- 3.1/10.0 MB 453.5 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 3.4/10.0 MB 462.2 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 3.7/10.0 MB 484.1 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 3.7/10.0 MB 484.1 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 3.9/10.0 MB 504.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 4.2/10.0 MB 522.5 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 538.3 kB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 538.3 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 4.7/10.0 MB 547.8 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 4.7/10.0 MB 547.8 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 4.7/10.0 MB 547.8 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 5.0/10.0 MB 536.1 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 5.0/10.0 MB 536.1 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 5.2/10.0 MB 539.3 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 5.2/10.0 MB 539.3 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 5.5/10.0 MB 546.1 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 5.5/10.0 MB 546.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 5.8/10.0 MB 554.1 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 5.8/10.0 MB 554.1 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 6.3/10.0 MB 572.3 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 6.3/10.0 MB 572.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 6.8/10.0 MB 603.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 614.6 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 621.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 621.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 621.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 621.8 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 621.8 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 7.6/10.0 MB 600.3 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/10.0 MB 600.3 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/10.0 MB 600.3 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/10.0 MB 600.3 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.9/10.0 MB 581.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.9/10.0 MB 581.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.1/10.0 MB 577.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 8.4/10.0 MB 550.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/10.0 MB 550.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/10.0 MB 550.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/10.0 MB 550.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 536.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 536.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 536.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 531.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 531.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 531.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 531.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.2/10.0 MB 524.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.2/10.0 MB 524.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.2/10.0 MB 524.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.2/10.0 MB 524.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.4/10.0 MB 511.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 476.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 417.1 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 666.0 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.4 MB 666.0 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.4 MB 666.0 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.4 MB 666.0 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.8/2.4 MB 431.0 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.8/2.4 MB 431.0 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.8/2.4 MB 431.0 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.8/2.4 MB 431.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 414.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 414.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 414.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 414.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 414.0 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.3/2.4 MB 350.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.3/2.4 MB 350.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.3/2.4 MB 350.6 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.6/2.4 MB 358.7 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.8/2.4 MB 320.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 298.3 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.4 MB 282.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.4 MB 282.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 282.6 kB/s eta 0:00:00\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.3.0 huggingface-hub-0.29.3 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375d0324-d850-4d99-84fc-3d4743f02be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDS:  [101, 24705, 1204, 17095, 1942, 1110, 170, 1846, 2235, 1872, 1118, 3353, 1592, 2240, 117, 1359, 1113, 113, 9066, 15306, 11689, 134, 3972, 13809, 23763, 114, 4220, 119, 102]\n",
      "Tokens: ['[CLS]', 'Cha', '##t', '##GP', '##T', 'is', 'a', 'language', 'model', 'developed', 'by', 'Open', '##A', '##I', ',', 'based', 'on', '(', 'Gene', '##rative', 'Pre', '=', 'trained', 'Trans', '##former', ')', 'architecture', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "text = '''ChatGPT is a language model developed by OpenAI, based on (Generative Pre=trained Transformer) architecture.'''\n",
    "\n",
    "encode = tokenizer.encode(text)\n",
    "\n",
    "print(\"Token IDS: \",encode)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encode)\n",
    "\n",
    "print(\"Tokens:\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795a5d24-edd2-4200-9809-3c7ccd3f4b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM,AutoTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "# Input text\n",
    "text = \"Transformers are amazing for NLP!\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_token_id = torch.argmax(logits[0,0]).item()\n",
    "print(\"Predicted Token ID:\", predicted_token_id)\n",
    "predicted_token=tokenizer.decode([predicted_token_id])\n",
    "print(\"Predicted token:\",predicted_token)\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35e31ee-310a-405e-a2c7-3de1cc7daf2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive me two random facts about India.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Make the API request using openai directly\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the latest model\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# Use chat_completion instead of response\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Store securely, but use it to initialize the OpenAI client\n",
    "openai.api_key = \"ENTER YOUR API KEY\"\n",
    "\n",
    "# Define the chat message properly\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me two random facts about India.\"}\n",
    "]\n",
    "\n",
    "# Make the API request using openai directly\n",
    "chat_completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Use the latest model\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(chat_completion[\"choices\"][0][\"message\"][\"content\"]) # Use chat_completion instead of response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ccba2b-83b2-4f9d-87d5-f01f0bcced8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
